{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming Exercise 8: Recommender Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, you will implement collaborative filtering to build a recommender system for movies. Before starting on the programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics. The files included in this assignment are: \n",
    "\n",
    "- ex8 movies.mat - Movie Review Dataset\n",
    "- ex8 movieParams.mat - Parameters provided for debugging\n",
    "- multivariateGaussian.m - Computes the probability density function for a Gaussian distribution\n",
    "- visualizeFit.m - 2D plot of a Gaussian distribution and a dataset\n",
    "- checkCostFunction.m - Gradient checking for collaborative filtering \n",
    "- computeNumericalGradient.m - Numerically compute gradients\n",
    "- fmincg.m - Function minimization routine (similar to fminunc) \n",
    "- loadMovieList.m - Loads the list of movies into a cell-array\n",
    "- movie ids.txt - List of movies\n",
    "- normalizeRatings.m - Mean normalization for collaborative filtering\n",
    "\n",
    "You only have to implement one functions:\n",
    "\n",
    "- cofiCostFunc.m - Implement the cost function for collaborative filtering\n",
    "\n",
    "The contained files are found in File --> Open... We highly recommend that you take a look at them as you make progress in this exercise. \n",
    "\n",
    "### NOTE:\n",
    "\n",
    "You will find cells which contain the comment % GRADED FUNCTION: functionName. Do not edit that comment. Those cells will be used to grade your assignment. Each block of code with that comment should only have the function. \n",
    "\n",
    "Instructions will be provided as needed in the exercise. \n",
    "\n",
    "\n",
    "#### When you are done and submit the assignment, click here to check your [submission](https://www.coursera.org/learn/ml-test-jupyter/programming/FXGDt). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recommender Systems\n",
    "===================\n",
    "\n",
    "In here, you will implement the collaborative\n",
    "filtering learning algorithm and apply it to a dataset of movie\n",
    "ratings. This dataset consists of ratings on a scale of 1 to 5. The\n",
    "dataset has $n_u = 943$ users, and $n_m= 1682$ movies. \n",
    "\n",
    "In the next parts of this exercise, you will implement the function\n",
    "cofiCostFunc that computes the collaborative fitlering\n",
    "objective function and gradient. After implementing the cost function\n",
    "and gradient, you will use fmincg to learn the parameters for\n",
    "collaborative filtering.\n",
    "\n",
    "Movie ratings dataset\n",
    "---------------------\n",
    "\n",
    "The first part below will load the dataset\n",
    "ex8_movies.mat, providing the variables Y and R in\n",
    "your Octave/MATLAB environment.\n",
    "\n",
    "The matrix $Y$ (a num_movies $\\times$ num_users matrix)\n",
    "stores the ratings $y^{(i,j)}$ (from 1 to 5). The matrix $R$ is an\n",
    "binary-valued indicator matrix, where $R(i,j) = 1$ if user $j$ gave a\n",
    "rating to movie $i$, and $R(i,j)=0$ otherwise. The objective of\n",
    "collaborative filtering is to predict movie ratings for the movies that\n",
    "users have not yet rated, that is, the entries with $R(i,j) = 0$. This\n",
    "will allow us to recommend the movies with the highest predicted ratings\n",
    "to the user.\n",
    "\n",
    "To help you understand the matrix Y, the cell\n",
    "below will compute the average movie rating for the first\n",
    "movie (Toy Story) and output the average rating to the screen.\n",
    "\n",
    "Throughout this part of the exercise, you will also be working with the\n",
    "matrices, X and Theta: \n",
    "\n",
    "$$X = \n",
    "\\begin{bmatrix}\n",
    "--- (x^{(1)})^T --- \\\\\n",
    "--- (x^{(2)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (x^{(n_m)})^T --- \\\\\n",
    "\\end{bmatrix} , \\quad\n",
    "{\\tt Theta} = \n",
    "\\begin{bmatrix}\n",
    "--- (\\theta^{(1)})^T --- \\\\\n",
    "--- (\\theta^{(2)})^T --- \\\\\n",
    "\\vdots \\\\\n",
    "--- (\\theta^{(n_u)})^T --- \\\\\n",
    "\\end{bmatrix}.$$ \n",
    "\n",
    "The $i$-th row of X corresponds to the\n",
    "feature vector $x^{(i)}$ for the $i$-th movie, and the $j$-th row of\n",
    "Theta corresponds to one parameter vector $\\theta^{(j)}$, for the\n",
    "$j$-th user. Both $x^{(i)}$ and $\\theta^{(j)}$ are $n$-dimensional\n",
    "vectors. For the purposes of this exercise, you will use $n=100$, and\n",
    "therefore, $x^{(i)}\\in\\mathbb{R}^{100}$ and\n",
    "$\\theta^{(j)}\\in\\mathbb{R}^{100}$. Correspondingly, X is a\n",
    "$n_m \\times 100$ matrix and Theta is a $n_u \\times 100$ matrix.\n",
    "\n",
    "We will start by loading the movie ratings dataset to understand the structure of the data.\n",
    "\n",
    "Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies on 943 users\n",
    "\n",
    "R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a rating to movie i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warning('off'); addpath('readonly');\n",
    "load ('ex8_movies.mat');         %  Load data\n",
    "\n",
    "%  From the matrix, we can compute statistics like average rating.\n",
    "fprintf('Average rating for movie 1 (Toy Story): %f / 5\\n\\n', ...\n",
    "        mean(Y(1, R(1, :))));\n",
    "\n",
    "imagesc(Y);             %  We can \"visualize\" the ratings matrix by plotting it with imagesc\n",
    "ylabel('Movies');\n",
    "xlabel('Users');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering learning algorithm\n",
    "------------------------------------------\n",
    "\n",
    "Now, you will start implementing the collaborative filtering learning\n",
    "algorithm. You will start by implementing the cost function (without\n",
    "regularization).\n",
    "\n",
    "The collaborative filtering algorithm in the setting of movie\n",
    "recommendations considers a set of $n$-dimensional parameter vectors\n",
    "$x^{(1)},...,x^{(n_m)}$ and $\\theta^{(1)},...,\\theta^{(n_u)}$, where the\n",
    "model predicts the rating for movie $i$ by user $j$ as\n",
    "$y^{(i,j)} = (\\theta^{(j)})^Tx^{(i)}$. Given a dataset that consists of\n",
    "a set of ratings produced by some users on some movies, you wish to\n",
    "learn the parameter vectors $x^{(1)},...,x^{(n_m)},\n",
    "\\theta^{(1)},...,\\theta^{(n_u)}$ that produce the best fit (minimizes\n",
    "the squared error).\n",
    "\n",
    "You will complete the code in cofiCostFunc to compute the cost\n",
    "function and gradient for collaborative filtering. Note that the\n",
    "parameters to the function (i.e., the values that you are trying to\n",
    "learn) are $X$ and $Theta$. In order to use an off-the-shelf\n",
    "minimizer such as fmin, the cost function has been set up to\n",
    "unroll the parameters into a single vector **params**. You had\n",
    "previously used the same vector unrolling method in the neural networks\n",
    "programming exercise.\n",
    "\n",
    "### Collaborative filtering cost function\n",
    "\n",
    "The collaborative filtering cost function (without regularization) is\n",
    "given by\n",
    "\n",
    "$$J({x^{(1)},...,x^{(n_m)}, \\theta^{(1)},...,\\theta^{(n_u)}})= \\frac{1}{2}\\sum_{(i,j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2.\\tag{1}$$\n",
    "\n",
    "You should now modify cofiCostFunc to return this cost in the\n",
    "variable. Note that you should be accumulating the cost for\n",
    "user $j$ and movie $i$ only if $R(i,j) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**: \n",
    " \n",
    "[J, grad] = **COFICOSTFUNC**(params, Y, R, num_users, num_movies, num_features, lambda) returns the cost and gradient for the collaborative filtering problem. You should compute the cost function and gradient for collaborative\n",
    "              filtering. Concretely, you should first implement the cost\n",
    "              function (without regularization) and make sure it is\n",
    "              matches our costs (equation 1)\n",
    "              . After that, you should implement the \n",
    "              gradient and use the checkCostFunction routine to check\n",
    "              that the gradient is correct (equations 2 and 3). Finally, you should implement regularization (equations 6 7 and 8).\n",
    "\n",
    "**X** = num_movies  x num_features matrix of movie features\n",
    "       \n",
    "**Theta** = num_users  x num_features matrix of user features\n",
    "\n",
    "**Y** = num_movies x num_users matrix of user ratings of movies\n",
    "\n",
    "**R** = num_movies x num_users matrix, where R(i, j) = 1 if the i-th movie was rated by the j-th user\n",
    "\n",
    "You should set the following variables correctly:\n",
    "\n",
    "**X_grad** = num_movies x num_features matrix, containing the \n",
    "                partial derivatives w.r.t. to each element of X\n",
    "       \n",
    "**Theta_grad** = num_users x num_features matrix, containing the \n",
    "                    partial derivatives w.r.t. to each element of Theta\n",
    "\n",
    "\n",
    "<table border = \"0\" width = \"550\" ><tr><td> \n",
    "\n",
    "**Implementation Note**: We strongly encourage you to use a vectorized implementation to compute\n",
    "$J$, since it will later by called many times by the optimization\n",
    "package fmincg. As usual, it might be easiest to first write a\n",
    "non-vectorized implementation (to make sure you have the right answer),\n",
    "and the modify it to become a vectorized implementation (checking that\n",
    "the vectorization steps don’t change your algorithm’s output). To come\n",
    "up with a vectorized implementation, the following tip might be helpful:\n",
    "You can use the R matrix to set selected entries to 0. For\n",
    "example, R .\\* M will do an element-wise multiplication between\n",
    "M and R; since R only has elements with values\n",
    "either 0 or 1, this has the effect of setting the elements of M\n",
    "to 0 only when the corresponding value in R is 0. Hence,\n",
    "sum(sum(R.\\*M)) is the sum of all the elements of M for\n",
    "which the corresponding element in R equals 1.  <br /> \n",
    "\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: cofiCostFunc\n",
    "function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ...\n",
    "                                  num_features, lambda)\n",
    "\n",
    "\n",
    "% Unfold the U and W matrices from params\n",
    "X = reshape(params(1:num_movies*num_features), num_movies, num_features);\n",
    "Theta = reshape(params(num_movies*num_features+1:end), ...\n",
    "                num_users, num_features);\n",
    "\n",
    "            \n",
    "% You need to return the following values correctly\n",
    "J = 0;\n",
    "X_grad = zeros(size(X));\n",
    "Theta_grad = zeros(size(Theta));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "% =============================================================\n",
    "\n",
    "grad = [X_grad(:); Theta_grad(:)];\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Calculate your cost\n",
    "load ('ex8_movieParams.mat');      %  Load pre-trained weights (X, Theta, num_users, num_movies, num_features)\n",
    "\n",
    "num_users = 4; num_movies = 5; num_features = 3;    %  Reduce the data set size so that this runs faster\n",
    "X = X(1:num_movies, 1:num_features);\n",
    "Theta = Theta(1:num_users, 1:num_features);\n",
    "Y = Y(1:num_movies, 1:num_users);\n",
    "R = R(1:num_movies, 1:num_users);\n",
    "\n",
    "J = cofiCostFunc([X(:) ; Theta(:)], Y, R, num_users, num_movies, ...   %  Evaluate cost function\n",
    "               num_features, 0);\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Final Regularized Cost**:\n",
    "\n",
    "Cost at loaded parameters is\n",
    "$22.22$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering gradient\n",
    "\n",
    "Now, you should implement the gradient (without regularization).\n",
    "Specifically, you should complete the code in *cofiCostFunc* to\n",
    "return the variables X_grad and Theta_grad. Note that\n",
    "X_grad should be a matrix of the same size as $X$ and\n",
    "similarly, Theta_grad is a matrix of the same size as\n",
    "Theta. The gradients of the cost function is given by:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x_k^{(i)}} = \\sum_{j:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})\\theta_k^{(j)}\\tag{2}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_k^{(j)}} = \\sum_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})x_k^{(i)}.\\tag{3}$$\n",
    "\n",
    "Note that the function returns the gradient for both sets of variables\n",
    "by unrolling them into a single vector. After you have completed the\n",
    "code to compute the gradients, we will run a\n",
    "gradient check (*checkCostFunction*) to numerically check the\n",
    "implementation of your gradients. If your implementation is correct,\n",
    "you should find that the analytical and numerical gradients match up\n",
    "closely.\n",
    "\n",
    "<table border = \"0\" width = \"550\" ><tr><td> \n",
    "\n",
    "**Implementation Note:** You can get full credit for this assignment without\n",
    "using a vectorized implementation, but your code will run much more\n",
    "slowly (a small number of hours), and so we recommend that you try to\n",
    "vectorize your implementation.\n",
    "To get started, you can implement the gradient with a for-loop over\n",
    "movies (for computing $\\frac{\\partial J}{\\partial x_k^{(i)}}$) and a\n",
    "for-loop over users (for computing $\\frac{\\partial J}{\\partial\n",
    "\\theta_k^{(j)}}$). When you first implement the gradient, you might\n",
    "start with an unvectorized version, by implementing another inner\n",
    "for-loop that computes each element in the summation. After you have\n",
    "completed the gradient computation this way, you should try to vectorize\n",
    "your implementation (vectorize the inner for-loops), so that you’re left\n",
    "with only two for-loops (one for looping over movies to compute\n",
    "$\\frac{\\partial J}{\\partial x_k^{(i)}}$ for each movie, and one for\n",
    "looping over users to compute\n",
    "$\\frac{\\partial J}{\\partial \\theta_k^{(j)}}$ for each user).  <br /> \n",
    "\n",
    "</td></tr></table>\n",
    "\n",
    "**Implementation Tip:** To perform the vectorization, you might find this helpful: You should come up\n",
    "with a way to compute all the derivatives associated with \n",
    "$x_1^{(i)}, x_2^{(i)}, \\ldots, x_n^{(i)}$ (i.e., the derivative terms associated \n",
    "with the feature vector $x^{(i)}$) at the same time.  Let us define the derivatives\n",
    "for the feature vector of the $i$-th movie as:\n",
    "\n",
    "$$(X_{grad}(i, :))^T = \\begin{bmatrix}\n",
    "\\frac{ \\partial J}{ \\partial x_1^{(i)}} \\\\\n",
    "\\frac{\\partial J}{\\partial x_2^{(i)}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{ \\partial J}{ \\partial x_n^{(i)}}\n",
    "\\end{bmatrix}=\n",
    "\\sum_{j:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)}) \\theta^{(j)}\\tag{4}$$\n",
    "\n",
    "To vectorize the above expression, you can start by indexing into $Theta$ and\n",
    "$Y$ to select only the elements of interests (that is, those with $r(i,j)=1$).\n",
    "Intuitively, when you consider the features for the $i$-th movie, you only need\n",
    "to be concern about the users who had given ratings to the movie, and this allows\n",
    "you to remove all the other users from Theta and Y. \n",
    "\n",
    "Concretely, you can set idx = find(R(i, :)==1) to be a list of all the \n",
    "users that have rated movie $i$. This will allow you to create the temporary \n",
    "matrices ${\\tt Theta_{temp} = Theta(idx, :)}$ and $Y_{temp} = Y(i, idx)$\n",
    "that index into  Theta and Y to give you only the set of users which \n",
    "have rated the $i$-th movie. This will allow you to write the derivatives as:\n",
    "\n",
    "$$ X_{grad}(i, :) = (X(i, :) * Theta_{temp}^T - Y_{temp}) * Theta_{temp}\\tag{5}$$\n",
    "\n",
    "(Note: The vectorized computation above returns a row-vector instead.) \n",
    "\n",
    "After you have vectorized the computations of the derivatives with respect to $x^{(i)}$,\n",
    "you should use a similar method to vectorize the derivatives with respect to\n",
    "$\\theta^{(j)}$ as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Checking if the grad argument in your cofiCostFunc is working\n",
    "checkCostFunction;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized cost function\n",
    "\n",
    "The cost function for collaborative filtering with regularization is\n",
    "given by\n",
    "\n",
    "$$\n",
    "J({x^{(1)},...,x^{(n_m)}, \\theta^{(1)},...,\\theta^{(n_u)}}) = \n",
    " \\frac{1}{2}\\sum_{(i,j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \n",
    " \\left(\\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta_k^{(j)})^2\\right)\n",
    "+ \\left(\\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^{n}(x_k^{(i)})^2\\right).\\tag{6}$$\n",
    "\n",
    "You should now add regularization to your original computations of the\n",
    "cost function, $J$. After you are done, run the cell below to check your regularized cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%  Evaluate cost function\n",
    "J = cofiCostFunc([X(:) ; Theta(:)], Y, R, num_users, num_movies, ...\n",
    "               num_features, 1.5)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "31.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized gradient\n",
    "\n",
    "Now that you have implemented the regularized cost function, you should\n",
    "proceed to implement regularization for the gradient. You should add to\n",
    "your implementation in *cofiCostFunc* to return the regularized\n",
    "gradient by adding the contributions from the regularization terms. Note\n",
    "that the gradients for the regularized cost function is given by:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x_k^{(i)}} = \\sum_{j:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})\\theta_k^{(j)} + \\lambda x_k^{(i)}\\tag{7}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_k^{(j)}} = \\sum_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)} - y^{(i,j)})x_k^{(i)} + \\lambda \\theta_k^{(j)}\\tag{8}$$\n",
    "\n",
    "This means that you just need to add $\\lambda x^{(i)}$ to the\n",
    "X_grad(i,:) variable described earlier, and add\n",
    "$\\lambda \\theta^{(j)}$ to the Theta_grad(j,:) variable described\n",
    "earlier.\n",
    "\n",
    "After you have completed the code to compute the gradients, run the cell below for another gradient check\n",
    "(checkCostFunction) to numerically check the implementation of your gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%  Check gradients by running checkNNGradients\n",
    "checkCostFunction(1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning movie recommendations\n",
    "------------------------------\n",
    "\n",
    "After you have finished implementing the collaborative filtering cost\n",
    "function and gradient, you can now start training your algorithm to make\n",
    "movie recommendations for yourself. In the cell below, you can enter your own movie preferences, so\n",
    "that later when the algorithm runs, you can get your own movie\n",
    "recommendations! We have filled out some values according to our own\n",
    "preferences, but you should change this according to your own tastes.\n",
    "The list of all movies and their number in the dataset can be found\n",
    "listed in the file movie_idx.txt.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "<table border = \"0\" width = \"440\" ><tr><td> \n",
    "Top recommendations for you: <br>\n",
    "Predicting rating 9.0 for movie Titanic (1997)  <br> \n",
    "Predicting rating 8.9 for movie Star Wars (1977)<br>\n",
    "Predicting rating 8.8 for movie Shawshank Redemption, The (1994)<br>\n",
    "Predicting rating 8.5 for movie As Good As It Gets (1997)<br>\n",
    "Predicting rating 8.5 for movie Good Will Hunting (1997)<br>\n",
    "Predicting rating 8.5 for movie Usual Suspects, The (1995)<br>\n",
    "Predicting rating 8.5 for movie Schindler's List (1993)<br>\n",
    "Predicting rating 8.4 for movie Raiders of the Lost Ark (1981)<br>\n",
    "Predicting rating 8.4 for movie Empire Strikes Back, The (1980)<br>\n",
    "Predicting rating 8.4 for movie Braveheart (1995)<br>\n",
    " <br>\n",
    "Original ratings provided:   <br>\n",
    "Rated 4 for Toy Story (1995)  <br>\n",
    "Rated 3 for Twelve Monkeys (1995) <br>\n",
    "Rated 5 for Usual Suspects, The (1995) <br>\n",
    "Rated 4 for Outbreak (1995)  <br>\n",
    "Rated 5 for Shawshank Redemption, The (1994)  <br>\n",
    "Rated 3 for While You Were Sleeping (1995) <br>\n",
    "Rated 5 for Forrest Gump (1994) <br>\n",
    "Rated 2 for Silence of the Lambs, The (1991)  <br>\n",
    "Rated 4 for Alien (1979)  <br>\n",
    "Rated 5 for Die Hard 2 (1990)  <br>\n",
    "Rated 5 for Sphere (1998)  \n",
    "<caption><center> Figure 4 </center> </caption>\n",
    "</td></tr></table>\n",
    "\n",
    "After the additional ratings have been added to the dataset, the script\n",
    "will proceed to train the collaborative filtering model. This will learn\n",
    "the parameters X and Theta. To predict the rating of movie\n",
    "$i$ for user $j$, you need to compute $(\\theta^{(j)})^Tx^{(i)}$. The\n",
    "next part of the script computes the ratings for all the movies and\n",
    "users and displays the movies that it recommends (Figure\n",
    "4), according to ratings that were entered earlier in\n",
    "the script. Note that you might obtain a different set of the\n",
    "predictions due to different random initializations.\n",
    "\n",
    "\n",
    "Before we will train the collaborative filtering model, we will first\n",
    "add ratings that correspond to a new user that we just observed. This\n",
    "part of the code will also allow you to put in your own ratings for the\n",
    "movies in our dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movieList = loadMovieList();\n",
    "\n",
    "my_ratings = zeros(1682, 1);          %  Initialize my ratings\n",
    "\n",
    "% Check the file movie_idx.txt for id of each movie in our dataset\n",
    "% For example, Toy Story (1995) has ID 1, so to rate it \"4\", you can set\n",
    "my_ratings(1) = 4;\n",
    "\n",
    "% Or suppose did not enjoy Silence of the Lambs (1991), you can set\n",
    "my_ratings(98) = 2;\n",
    "\n",
    "% We have selected a few movies we liked / did not like and the ratings we\n",
    "% gave are as follows:\n",
    "my_ratings(7) = 3;\n",
    "my_ratings(12)= 5;\n",
    "my_ratings(54) = 4;\n",
    "my_ratings(64)= 5;\n",
    "my_ratings(66)= 3;\n",
    "my_ratings(69) = 5;\n",
    "my_ratings(183) = 4;\n",
    "my_ratings(226) = 5;\n",
    "my_ratings(355)= 5;\n",
    "\n",
    "fprintf('\\n\\nNew user ratings:\\n');\n",
    "for i = 1:length(my_ratings)\n",
    "    if my_ratings(i) > 0 \n",
    "        fprintf('Rated %d for %s\\n', my_ratings(i), ...\n",
    "                 movieList{i});\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will train the collaborative filtering model on a movie rating dataset of 1682 movies and 943 users. Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies by  943 users. R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a rating to movie i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load('ex8_movies.mat');      %  Load data\n",
    "\n",
    "%  Add our own ratings to the data matrix\n",
    "Y = [my_ratings Y];\n",
    "R = [(my_ratings ~= 0) R];\n",
    "\n",
    "%  Normalize Ratings\n",
    "[Ynorm, Ymean] = normalizeRatings(Y, R);\n",
    "\n",
    "%  Useful Values\n",
    "num_users = size(Y, 2);\n",
    "num_movies = size(Y, 1);\n",
    "num_features = 10;\n",
    "\n",
    "% Set Initial Parameters (Theta, X)\n",
    "X = randn(num_movies, num_features);\n",
    "Theta = randn(num_users, num_features);\n",
    "\n",
    "initial_parameters = [X(:); Theta(:)];\n",
    "\n",
    "% Set options for fmincg\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 100);\n",
    "\n",
    "% Set Regularization\n",
    "lambda = 1;\n",
    "theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, ...\n",
    "                                num_features, lambda)), ...\n",
    "                initial_parameters, options);\n",
    "\n",
    "% Unfold the returned theta back into U and W\n",
    "X = reshape(theta(1:num_movies*num_features), num_movies, num_features);\n",
    "Theta = reshape(theta(num_movies*num_features+1:end), ...\n",
    "                num_users, num_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could compute our prediction matrix!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = X * Theta';\n",
    "my_predictions = p(:,1) + Ymean;\n",
    "\n",
    "movieList = loadMovieList();\n",
    "\n",
    "[r, ix] = sort(my_predictions, 'descend');\n",
    "fprintf('\\nTop recommendations for you:\\n');\n",
    "for i=1:10\n",
    "    j = ix(i);\n",
    "    fprintf('Predicting rating %.1f for movie %s\\n', my_predictions(j), ...\n",
    "            movieList{j});\n",
    "end\n",
    "\n",
    "fprintf('\\n\\nOriginal ratings provided:\\n');\n",
    "for i = 1:length(my_ratings)\n",
    "    if my_ratings(i) > 0 \n",
    "        fprintf('Rated %d for %s\\n', my_ratings(i), ...\n",
    "                 movieList{i});\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "ml-test-jupyter",
   "graded_item_id": "FXGDt",
   "launcher_item_id": "AMiKc",
   "submission_attachments": [
    "computeCost.m",
    "computeCostMulti.m",
    "featureNormalize.m",
    "gradientDescent.m",
    "gradientDescentMulti.m",
    "normalEqn.m",
    "plotData.m",
    "warmUpExercise.m"
   ]
  },
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
