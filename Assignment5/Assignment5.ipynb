{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming Exercise 5: Regularized Linear Regression and Bias v.s. Variance\n",
    "\n",
    "\n",
    "In this exercise, you will implement regularized linear regression and use it to study models with different bias-variance properties. Before starting on the programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics.\n",
    "\n",
    "Throughout this exercise you have to complete the following functions: \n",
    "\n",
    "- linearRegCostFunction.m - Regularized linear regression cost function\n",
    "- learningCurve.m - Generates a learning curve\n",
    "- polyFeatures.m - Maps data into polynomial feature space \n",
    "- validationCurve.m - Generates a cross validation curve\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "For each function, we have provided starter code for you. You will find a description of the problems followed by a cell which contains some code. You will have to write your own code in these cells to complete the four functions mentioned above. Once you run the cells, your output would be displayed. If it matches the expected output, then you should continue with the next part of the exercise, else, you would have to try again. \n",
    "\n",
    "Please read the [Notebook FAQ](https://www.coursera.org/learn/ml-test-jupyter/supplement/Zr7s1/jupyter-notebook-faq) to get familiar with the Jupyter environment and the commands. We also highly recommend going to your workspace to explore the files you will be working with before starting the exercise.\n",
    "\n",
    "To go to the workspace: press on File ==> Open.\n",
    "\n",
    "### NOTE:\n",
    "You will find cells which contain the comment % GRADED FUNCTION: functionName. Do not edit that comment. Those cells will be used to grade your assignment. Each block of code with that comment should only have the function. \n",
    "\n",
    "\n",
    "#### After submitting your assignment, you can [check your grades here](https://www.coursera.org/learn/ml-test-jupyter/programming/9k6Rg). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "In the first half of the exercise, you will implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir. In the next half, you will go through some diagnostics of debugging learning algorithms and examine the effects of bias v.s. variance.\n",
    "You will be provided with instructions as you complete this exercise. \n",
    "Visualizing the dataset\n",
    "-----------------------\n",
    "\n",
    "We will begin by visualizing the dataset containing historical records\n",
    "on the change in the water level, $x$, and the amount of water flowing\n",
    "out of the dam, $y$.\n",
    "\n",
    "This dataset is divided into three parts:\n",
    "\n",
    "-   A **training** set that your model will learn on: $X, y$\n",
    "\n",
    "-   A **cross validation** set for determining the regularization\n",
    "    parameter: $X$val, $y$val\n",
    "\n",
    "-   A **test** set for evaluating performance. These are “unseen”\n",
    "    examples which your model did not see during training: $Xtest,\n",
    "    ytest$\n",
    "\n",
    "Below, we will plot the training data (Figure\n",
    "1). In the following parts, you will implement linear\n",
    "regression and use that to fit a straight line to the data and plot\n",
    "learning curves. Following that, you will implement polynomial\n",
    "regression to find a better fit to the data.\n",
    "\n",
    "<img src=\"figure 1.png\" width=\"450\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load ('ex5data1.mat');\n",
    "m = size(X, 1);                                         % m = Number of examples\n",
    "\n",
    "plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);   % Plot training data\n",
    "xlabel('Change in water level (x)');\n",
    "ylabel('Water flowing out of the dam (y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression \n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "\n",
    "Recall that regularized linear regression has the following cost\n",
    "function:\n",
    "\n",
    "$$J(\\theta)  =  \\frac{{1}}{2m}\\left(\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}\\right)+\\frac{\\lambda}{2m}\\left(\\sum_{j=1}^{n}\\theta_{j}^{2}\\right)\\tag{1}$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter which controls the degree\n",
    "of regularization (helps prevent overfitting) and $h(\\theta) = X\\theta$. The\n",
    "regularization term puts a penalty on the overal cost $J$. As the\n",
    "magnitudes of the model parameters $\\theta_j$ increase, the penalty\n",
    "increases as well. **Note that you should not regularize the $\\theta_0$\n",
    "term.** (In Octave/MATLAB, the $\\theta_0$ term is represented as\n",
    "theta(1) since indexing in Octave/MATLAB starts from 1).\n",
    "\n",
    "You should now complete the code in the\n",
    "linearRegCostFunction. Your task is to write a function to\n",
    "calculate the regularized linear regression cost function. If possible,\n",
    "try to vectorize your code and avoid writing loops. When you are\n",
    "finished, we will run your cost function\n",
    "using theta initialized at [1; 1]. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient\n",
    "\n",
    "Correspondingly, the partial derivative of regularized linear\n",
    "regression’s cost for $\\theta_j$ is defined as\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial\\theta_0} = \\frac{1}{m}  \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} \\quad\\quad\\quad\\quad\\quad\\quad \\mbox{for $j = 0$}\\tag{2}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial\\theta_j} = \\left( \\frac{1}{m}  \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j  \\quad\\, \\mbox{for $j \\geq 1$}\\tag{3}$$\n",
    "\n",
    "In linearRegCostFunction, add code to calculate the gradient,\n",
    "returning it in the variable grad. When you are finished, the\n",
    "next part  will run your gradient function using\n",
    "theta initialized at [1; 1]. \n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "**[J, grad] = linearRegCostFunction(X, y, theta, lambda)** computes the \n",
    "cost of using theta as the parameter for linear regression to fit the \n",
    "data points in X and y. It returns the cost in J and the gradient in grad. You should set J = equation 1 and then you should set the grad(1) = equation 2 and the rest to equation 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: linearRegCostFunction\n",
    "function [J, grad] = linearRegCostFunction(X, y, theta, lambda)\n",
    "\n",
    "m = length(y);                         % number of training examples\n",
    "                                       % Return the following variables correctly \n",
    "J = 0;\n",
    "grad = zeros(size(theta));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "% ============================================================\n",
    "grad = grad(:);\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Evaluate the Cost and the gradient\n",
    "theta = [1 ; 1];\n",
    "[J grad] = linearRegCostFunction([ones(m, 1) X], y, theta, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Cost and Gradient**\n",
    "\n",
    "J = 303.993.\n",
    "\n",
    "grad = [-15.30; 598.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting linear regression\n",
    "-------------------------\n",
    "\n",
    "Once your cost function and gradient are working correctly, the next\n",
    "part will run the code in trainLinearReg to\n",
    "compute the optimal values of $\\theta$. This training function uses\n",
    "fmincg to optimize the cost function.\n",
    "\n",
    "In this part, we set regularization parameter $\\lambda$ to zero. Because\n",
    "our current implementation of linear regression is trying to fit a\n",
    "2-dimensional $\\theta$, regularization will not be incredibly helpful\n",
    "for a $\\theta$ of such low dimension. In the later parts of the\n",
    "exercise, you will be using polynomial regression with regularization.\n",
    "\n",
    "Finally, we will also plot the best fit line,\n",
    "resulting in an image similar to Figure 2. The best\n",
    "fit line tells us that the model is not a good fit to the data because\n",
    "the data has a non-linear pattern. While visualizing the best fit as\n",
    "shown is one possible way to debug your learning algorithm, it is not\n",
    "always easy to visualize the data and model. In the next section, you\n",
    "will implement a function to generate learning curves that can help you\n",
    "debug your learning algorithm even if it is not easy to visualize the\n",
    "data.\n",
    "<img src=\"figure 2.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "You do not have to write any code. The data is non-linear, so this will not give a great fit. Run the cell below to see a similar plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda = 0;                                             %  Train linear regression with lambda = 0\n",
    "[theta] = trainLinearReg([ones(m, 1) X], y, lambda);\n",
    "\n",
    "plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);   %  Plot fit over the data\n",
    "xlabel('Change in water level (x)');\n",
    "ylabel('Water flowing out of the dam (y)');\n",
    "hold on;\n",
    "plot(X, [ones(m, 1) X]*theta, '--', 'LineWidth', 2)\n",
    "hold off;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias-variance\n",
    "=============\n",
    "\n",
    "An important concept in machine learning is the bias-variance tradeoff.\n",
    "Models with high bias are not complex enough for the data and tend to\n",
    "underfit, while models with high variance overfit to the training data.\n",
    "\n",
    "In this part of the exercise, you will plot training and test errors on\n",
    "a learning curve to diagnose bias-variance problems.\n",
    "\n",
    "Learning curves\n",
    "---------------\n",
    "\n",
    "You will now implement code to generate the learning curves that will be\n",
    "useful in debugging learning algorithms. Recall that a learning curve\n",
    "plots training and cross validation error as a function of training set\n",
    "size. Your job is to fill in the learningCurve function so that it returns a\n",
    "vector of errors for the training set and the cross validation set.\n",
    "\n",
    "To plot the learning curve, we need a training and cross validation set\n",
    "error for different *training* set sizes. To obtain different training\n",
    "set sizes, you should use different subsets of the original training set\n",
    "$X$. Specifically, for a training set size of $i$, you should\n",
    "use the first i examples (i.e., $X(1:i,:)$ and $y(1:i)$).\n",
    "\n",
    "You can use the trainLinearReg function to find the $\\theta$\n",
    "parameters. Note that the **lambda** is passed as a parameter to the\n",
    "**learningCurve** function. After learning the $\\theta$ parameters,\n",
    "you should compute the **error** on the training and cross validation\n",
    "sets. Recall that the training error for a dataset is defined as\n",
    "\n",
    "$$J_{\\mathrm{train}}(\\theta)  =  \\frac{{1}}{2m}\\left[\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}\\right].$$\n",
    "\n",
    "In particular, note that the training error does not include the\n",
    "regularization term. One way to compute the training error is to use\n",
    "your existing cost function and set $\\lambda$ to 0 *only* when using it\n",
    "to compute the training error and cross validation error. When you are\n",
    "computing the training set error, make sure you compute it on the\n",
    "training subset (i.e., $X(1:n,:)$ and $y(1:n)$) (instead of\n",
    "the entire training set). However, for the cross validation error, you\n",
    "should compute it over the *entire* cross validation set. You should\n",
    "store the computed errors in the vectors **error_train** and\n",
    "**error_val.**\n",
    "\n",
    "When you are finished, we will print the learning curves and\n",
    "produce a plot similar to the image below.\n",
    "\n",
    "<img src=\"figure 3.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "You can observe that *both* the\n",
    "train error and cross validation error are high when the number of\n",
    "training examples is increased. This reflects a **high bias** problem in\n",
    "the model – the linear regression model is too simple and is unable to\n",
    "fit our dataset well. In the next section, you will implement polynomial\n",
    "regression to fit a better model for this dataset.\n",
    "\n",
    "**Implementation**: \n",
    "\n",
    "\n",
    "**[error_train, error_val] =\n",
    "       leanrningCurve(X, y, Xval, yval, lambda)** returns the train and\n",
    "       cross validation set errors for a learning curve. In particular, \n",
    "       it returns two vectors of the same length: **error_train** and \n",
    "       **error_val**. **error_train(i)** contains the training error for\n",
    "       i examples (and similarly for **error_val(i)**). In this function, you will compute the train and test errors for\n",
    "   dataset sizes from 1 up to m. In practice, when working with larger\n",
    "   datasets, you might want to do this in larger intervals. For the cross-validation error, you should instead evaluate on\n",
    "the _entire_ cross validation set ($X$val and $y$val). \n",
    "\n",
    "**Note:** To get your theta parameters, you should use the trainLinearReg function we have provided for you. You could scroll up to see how we used it. Once you have your theta, you could use your cost function (linearRegCostFunction)\n",
    "     to compute the training and cross validation error. You should \n",
    "     call the linearRegCostFunction with the lambda argument set to 0.\n",
    "     Do note that you will still need to use lambda when running\n",
    "     the training to obtain the theta parameters. \n",
    "\n",
    "You can loop over the examples with the following:\n",
    "\n",
    "      for i = 1:m\n",
    "          % Compute train/cross validation errors using training examples \n",
    "          % X(1:i, :) and y(1:i), storing the result in \n",
    "          % error_train(i) and error_val(i)\n",
    "          ....\n",
    "          \n",
    "      end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: learningCurve\n",
    "function [error_train, error_val] = ...\n",
    "    learningCurve(X, y, Xval, yval, lambda)\n",
    "\n",
    "% Number of training examples\n",
    "m = size(X, 1);\n",
    "\n",
    "% You need to return these values correctly\n",
    "error_train = zeros(m, 1);\n",
    "error_val   = zeros(m, 1);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Plotting your results\n",
    "warning(\"off\", \"all\");                         % Turn off the warnings to avoid fmincg errors - Don't worry about it! \n",
    "lambda = 0;\n",
    "[error_train, error_val] = ...\n",
    "learningCurve([ones(m, 1) X], y, ...\n",
    "                  [ones(size(Xval, 1), 1) Xval], yval, ...\n",
    "                  lambda);\n",
    "plot(1:m, error_train, 1:m, error_val);\n",
    "title('Learning curve for linear regression')\n",
    "legend('Train', 'Cross Validation')\n",
    "xlabel('Number of training examples')\n",
    "ylabel('Error')\n",
    "axis([0 13 0 150])\n",
    "\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression\n",
    "=====================\n",
    "\n",
    "The problem with our linear model was that it was too simple for the\n",
    "data and resulted in underfitting (high bias). In this part of the\n",
    "exercise, you will address this problem by adding more features.\n",
    "\n",
    "In the polynomial regression, our hypothesis has the form:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "h_\\theta(x) &= \\theta_0 + \\theta_1*(\\mathrm{waterLevel}) + \\theta_2* (\\mathrm{waterLevel})^2 + \\dots + \\theta_p*(\\mathrm{waterLevel})^p \\\\\n",
    "&= \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_p x_p.\\end{aligned}$$\n",
    "\n",
    "Notice that by defining\n",
    "$x_1 = (\\mathrm{waterLevel}), x_2=(\\mathrm{waterLevel})^2, \\dots, x_p = (\\mathrm{waterLevel})^p$,\n",
    "we obtain a linear regression model where the features are the various\n",
    "powers of the original value $(\\mathrm{waterLevel})$.\n",
    "\n",
    "Now, you will add more features using the higher powers of the existing\n",
    "feature $x$ in the dataset. Your task in this part is to complete the\n",
    "code in polyFeatures so that the function maps the original\n",
    "training set $X$ of size $m\\times1$ into its higher powers.\n",
    "Specifically, when a training set $X$ of size $m \\times 1$ is\n",
    "passed into the function, the function should return an $m \\times p$\n",
    "matrix $X$_poly, where column 1 holds the original values of\n",
    "$X$, column 2 holds the values of $X.^2$, column 3 holds the\n",
    "values of $X.^3$, and so on. Note that you don’t have to account\n",
    "for the zero-eth power in this function.\n",
    "\n",
    "Now you have a function that will map features to a higher dimension,\n",
    "and below we will apply it to the training set, the test\n",
    "set, and the cross validation set (which you haven’t used yet).\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "polyFeatures Maps X (1D vector) to the p-th power\n",
    "   [X_poly] = polyFeatures(X, p) takes a data matrix X (size m x 1) and\n",
    "   maps each example into its polynomial features where\n",
    "   \n",
    "   \n",
    "   $$X_{poly}(i, :) = [x_i \\quad x_i^2 \\quad x_i^3 \\quad ...  \\quad x_i^p]$$\n",
    "\n",
    "Instructions: Given a vector X, return a matrix X_poly where the p-th \n",
    "              column of X contains the values of X to the p-th power.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: polyFeatures\n",
    "function [X_poly] = polyFeatures(X, p)\n",
    "\n",
    "X_poly = zeros(numel(X), p);                             % You need to return the following variables correctly.\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Polynomial Regression\n",
    "------------------------------\n",
    "\n",
    "After you have completed polyFeatures, we\n",
    "will proceed to train polynomial regression using your linear regression\n",
    "cost function.\n",
    "\n",
    "Keep in mind that even though we have polynomial terms in our feature\n",
    "vector, we are still solving a linear regression optimization problem.\n",
    "The polynomial terms have simply turned into features that we can use\n",
    "for linear regression. We are using the same cost function and gradient\n",
    "that you wrote for the earlier part of this exercise.\n",
    "\n",
    "For this part of the exercise, you will be using a polynomial of degree\n",
    "8. It turns out that if we run the training directly on the projected\n",
    "data, it will not work well as the features would be badly scaled (e.g., an\n",
    "example with $x=40$ will now have a feature\n",
    "$x_8 = 40^8 = 6.5 \\times 10^{12}$). Therefore, you will need to use\n",
    "feature normalization.\n",
    "\n",
    "Before learning the parameters $\\theta$ for the polynomial regression,\n",
    "we will first call featureNormalize and normalize the\n",
    "features of the training set, storing the **mu, sigma** parameters\n",
    "separately. We have already implemented this function for you and it is\n",
    "the same function from the first exercise.\n",
    "\n",
    "After learning the parameters $\\theta$, you should see two plots (Figure\n",
    "4, 5) generated for polynomial regression with $\\lambda = 0$.\n",
    "\n",
    "<table><tr><td><img src=\"figure 4.png\" width=\"450\" height=\"450\"></td><td><img src=\"figure 5.png\" width=\"450\" height=\"450\"></td></tr></table>\n",
    "\n",
    "\n",
    "From Figure 4, you should see that the polynomial\n",
    "fit is able to follow the datapoints very well - thus, obtaining a low\n",
    "training error. However, the polynomial fit is very complex and even\n",
    "drops off at the extremes. Depending on which software you are using, the second tail of figure 4 could either drop as shown or go up in an exponential way. Either way, this is an indicator that the polynomial\n",
    "regression model is overfitting the training data and will not\n",
    "generalize well.\n",
    "\n",
    "To better understand the problems with the unregularized ($\\lambda=0$)\n",
    "model, you can see that the learning curve (Figure 5) shows the same effect where the low\n",
    "training error is low, but the cross validation error is high. There is\n",
    "a gap between the training and cross validation errors, indicating a\n",
    "high variance problem.\n",
    "\n",
    "One way to combat the overfitting (high-variance) problem is to add\n",
    "regularization to the model. In the next section, you will get to try\n",
    "different $\\lambda$ parameters to see how regularization can lead to a\n",
    "better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 8;                                           % Testing your function\n",
    "X_poly = polyFeatures(X, p);                     % Map X onto Polynomial Features and Normalize\n",
    "[X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize\n",
    "X_poly = [ones(m, 1), X_poly];                   % Add Ones\n",
    "\n",
    "                                                 % Map X_poly_test and normalize (using mu and sigma)\n",
    "X_poly_test = polyFeatures(Xtest, p);\n",
    "X_poly_test = bsxfun(@minus, X_poly_test, mu);\n",
    "X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);\n",
    "X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones\n",
    "\n",
    "X_poly_val = polyFeatures(Xval, p);               % Map X_poly_val and normalize (using mu and sigma)\n",
    "X_poly_val = bsxfun(@minus, X_poly_val, mu);\n",
    "X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);\n",
    "X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones\n",
    "\n",
    "fprintf('Normalized Training Example 1:\\n');\n",
    "fprintf('  %f  \\n', X_poly(1, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Regularization Parameter\n",
    "\n",
    "Now, you will get to experiment with polynomial regression with multiplevalues of lambda. The code below runs polynomial regression with lambda = 0. You should try running the code with different values oflambda to see how the fit and learning curve change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda = 1;\n",
    "[theta] = trainLinearReg(X_poly, y, lambda);\n",
    "\n",
    "figure('Position',[0,0,1000,400]);\n",
    "subplot (1, 2, 1)\n",
    "\n",
    "plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);\n",
    "plotFit(min(X), max(X), mu, sigma, theta, p);\n",
    "xlabel('Change in water level (x)');\n",
    "ylabel('Water flowing out of the dam (y)');\n",
    "title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));\n",
    "\n",
    "subplot (1, 2, 2)\n",
    "[error_train, error_val] = ...\n",
    "    learningCurve(X_poly, y, X_poly_val, yval, lambda);\n",
    "plot(1:m, error_train, 1:m, error_val);\n",
    "\n",
    "title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));\n",
    "xlabel('Number of training examples')\n",
    "ylabel('Error')\n",
    "axis([0 13 0 100])\n",
    "legend('Train', 'Cross Validation')\n",
    "\n",
    "lambda\n",
    "fprintf('# Training Examples\\tTrain Error\\tCross Validation Error\\n');\n",
    "for i = 1:m\n",
    "    fprintf('  \\t%d\\t\\t%f\\t%f\\n', i, error_train(i), error_val(i));\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional (ungraded) exercise: Adjusting the regularization parameter\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "In this section, you will get to observe how the regularization\n",
    "parameter affects the bias-variance of regularized polynomial\n",
    "regression. You should now modify the lambda parameter above and try $\\lambda = 1, 100$. For each of these values, the\n",
    "script should generate a polynomial fit to the data and also a learning\n",
    "curve.\n",
    "\n",
    "For $\\lambda=1$, you should see a polynomial fit that follows the data\n",
    "trend well (Figure 6) and a learning curve\n",
    "(Figure 7) showing that both the cross\n",
    "validation and training error converge to a relatively low value. This\n",
    "shows the $\\lambda=1$ regularized polynomial regression model does not\n",
    "have the high-bias or high-variance problems. In effect, it achieves a\n",
    "good trade-off between bias and variance.\n",
    "\n",
    "<table><tr><td><img src=\"figure 6.png\" width=\"450\" height=\"450\"></td><td><img src=\"figure 7.png\" width=\"450\" height=\"450\"></td><td><img src=\"figure 8.png\" width=\"450\" height=\"450\"></td></tr></table>\n",
    "\n",
    "For $\\lambda=100$, you should see a polynomial fit (Figure 8) that does not follow the data well. In this\n",
    "case, there is too much regularization and the model is unable to fit\n",
    "the training data. Change the lambda value in the cell above to see these graphs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting $\\lambda$ using a cross validation set\n",
    "------------------------------------------------\n",
    "\n",
    "From the previous parts of the exercise, you observed that the value of\n",
    "$\\lambda$ can significantly affect the results of regularized polynomial\n",
    "regression on the training and cross validation set. In particular, a\n",
    "model without regularization ($\\lambda=0$) fits the training set well,\n",
    "but does not generalize. Conversely, a model with too much\n",
    "regularization ($\\lambda = 100$) does not fit the training set and\n",
    "testing set well. A good choice of $\\lambda$ (e.g., $\\lambda=1$) can\n",
    "provide a good fit to the data.\n",
    "\n",
    "In this section, you will implement an automated method to select the\n",
    "$\\lambda$ parameter. Concretely, you will use a cross validation set to\n",
    "evaluate how good each $\\lambda$ value is. After selecting the best\n",
    "$\\lambda$ value using the cross validation set, we can then evaluate the\n",
    "model on the test set to estimate how well the model will perform on\n",
    "actual unseen data.\n",
    "\n",
    "Your task is to complete the code in the **validationCurve** function below.\n",
    "Specifically, you should use the **trainLinearReg** function\n",
    "to train the model using different values of $\\lambda$ and compute the\n",
    "training error and cross validation error. You should try $\\lambda$ in\n",
    "the following range: $\\{0, 0.001, 0.003, 0.01, 0.03,\n",
    "0.1, 0.3, 1, 3, 10\\}$.\n",
    "\n",
    "<img src=\"figure 9.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "After you have completed the code, the next part will\n",
    "run your function and plot a cross validation curve of error v.s.\n",
    "$\\lambda$ that allows you to select which $\\lambda$ parameter to use. You\n",
    "should see a plot similar to Figure 9. In this figure, we\n",
    "can see that the best value of $\\lambda$ is around 3. Due to randomness\n",
    "in the training and validation splits of the dataset, the cross\n",
    "validation error can sometimes be lower than the training error.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "validationCurve generates the train and validation errors needed to\n",
    "plot a validation curve that we can use to select lambda\n",
    "  ** [lambda_vec, error_train, error_val] = ...\n",
    "       VALIDATIONCURVE(X, y, Xval, yval)** returns the train\n",
    "       and validation errors (in error_train, error_val)\n",
    "       for different values of lambda. You are given the training set (X,\n",
    "       y) and validation set (Xval, yval).\n",
    "\n",
    "\n",
    "You have to fill in this function to return training errors in \n",
    "              error_train and the validation errors in error_val. The \n",
    "              vector lambda_vec contains the different lambda parameters \n",
    "              to use for each calculation of the errors, i.e, \n",
    "              error_train(i), and error_val(i) should give \n",
    "              you the errors obtained after training with \n",
    "              lambda = lambda_vec(i)\n",
    "              \n",
    "Note: You can loop over lambda_vec with the following:\n",
    "\n",
    "      for i = 1:length(lambda_vec)\n",
    "          lambda = lambda_vec(i);\n",
    "          % Compute train / val errors when training linear \n",
    "          % regression with regularization parameter lambda\n",
    "          % You should store the result in error_train(i)\n",
    "          % and error_val(i)\n",
    "          ....\n",
    "          \n",
    "      end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: validationCurve\n",
    "function [lambda_vec, error_train, error_val] = ...\n",
    "    validationCurve(X, y, Xval, yval)\n",
    "\n",
    "% Selected values of lambda (you should not change this)\n",
    "lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]';\n",
    "\n",
    "% You need to return these variables correctly.\n",
    "error_train = zeros(length(lambda_vec), 1);\n",
    "error_val = zeros(length(lambda_vec), 1);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "% =============================================================\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% Testing your function for various lambda values\n",
    "% to finally select the best lambda.\n",
    "\n",
    "[lambda_vec, error_train, error_val] = ...\n",
    "    validationCurve(X_poly, y, X_poly_val, yval);\n",
    "\n",
    "close all;\n",
    "plot(lambda_vec, error_train, lambda_vec, error_val);\n",
    "legend('Train', 'Cross Validation');\n",
    "xlabel('lambda');\n",
    "ylabel('Error');\n",
    "\n",
    "fprintf('lambda\\t\\tTrain Error\\tValidation Error\\n');\n",
    "for i = 1:length(lambda_vec)\n",
    "\tfprintf(' %f\\t%f\\t%f\\n', ...\n",
    "            lambda_vec(i), error_train(i), error_val(i));\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional (ungraded) exercise: Computing test set error\n",
    "------------------------------------------------------\n",
    "\n",
    "In the previous part of the exercise, you implemented code to compute\n",
    "the cross validation error for various values of the regularization\n",
    "parameter $\\lambda$. However, to get a better indication of the model’s\n",
    "performance in the real world, it is important to evaluate the “final”\n",
    "model on a test set that was not used in any part of training (that is,\n",
    "it was neither used to select the $\\lambda$ parameters, nor to learn the\n",
    "model parameters $\\theta$).\n",
    "\n",
    "For this optional (ungraded) exercise, you should compute the test error\n",
    "using the best value of $\\lambda$ you found. In our cross validation, we\n",
    "obtained a test error of 3.8599 for $\\lambda=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Use this cell to do it if you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional (ungraded) exercise: Plotting learning curves with randomly selected examples\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "In practice, especially for small training sets, when you plot learning\n",
    "curves to debug your algorithms, it is often helpful to average across\n",
    "multiple sets of randomly selected examples to determine the training\n",
    "error and cross validation error.\n",
    "\n",
    "Concretely, to determine the training error and cross validation error\n",
    "for $i$ examples, you should first randomly select $i$ examples from the\n",
    "training set and $i$ examples from the cross validation set. You will\n",
    "then learn the parameters $\\theta$ using the randomly chosen training\n",
    "set and evaluate the parameters $\\theta$ on the randomly chosen training\n",
    "set and cross validation set. The above steps should then be repeated\n",
    "multiple times (say 50) and the averaged error should be used to\n",
    "determine the training error and cross validation error for $i$\n",
    "examples.\n",
    "\n",
    "<img src=\"figure 10.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "For this optional (ungraded) exercise, you should implement the above\n",
    "strategy for computing the learning curves. For reference, figure\n",
    "10 shows the learning curve we obtained\n",
    "for polynomial regression with $\\lambda = 0.01$. Your figure may differ\n",
    "slightly due to the random selection of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% Use this cell to do it if you want\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "ml-test-jupyter",
   "graded_item_id": "9k6Rg",
   "launcher_item_id": "CU4HC",
   "submission_attachments": [
    "computeCost.m",
    "computeCostMulti.m",
    "featureNormalize.m",
    "gradientDescent.m",
    "gradientDescentMulti.m",
    "normalEqn.m",
    "plotData.m",
    "warmUpExercise.m"
   ]
  },
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
